\chapter{Methodology} \label{chap:methodology}
This chapter lays out the methodical steps I took while implementing the project.
Main steps for implementing this project fall into four general categories.
These four categories are:
\begin{itemize}
    \item Establishing a baseline benchmark
    \item Improving metrics beyond baseline benchmark
    \item Ensuring model interpretability is maintained
    \item Deploying the final model to production
\end{itemize}

Further sections will provide detail for these four main categories.

\section{Establishing a benchmark} \label{sec:estbench}
The first step to every machine learning project is that model produced is performs better than any random or naive solution.
Although random guess in any binary classification suggests that the minimum accuracy should be greater or equal than 50\%, because of the imbalance in the test dataset, minimum accuracy requirement for this dataset is even higher.
I have 390 pneumonia images in the 624 total images in the test dataset, dummy classifier that predict pneumonia for any given image will achieve 62.5\% accuracy in this test dataset.
Because of the imbalance in the dataset, accuracy would not be a good metric choice for assessing the performance.
Precision (specificity) and recall (sensitivity) are a better choice of performance metrics in the field of medical research.

% For that reason on any experiment conducted, collecting precision, recall as well as calculating f1 score will be utilized.
If I introduce these metrics in more detail,
precision is calculated by dividing true positives (tp) in predictions to true positives and false positives (fp).
\begin{equation}
    Precision = \frac{tp}{tp + fp}
\end{equation}

The recall is calculated by dividing true positives to true positives and false negatives (fn).

\begin{equation}
    Recall = \frac{tp}{tp + fn}
\end{equation}

To capture the correct performance of the classifier we need to consider both precision and recall.
Because I am aiming to reduce both false positive and false negatives rates
F1 score provides the ability to consider both precision and recall for the same classification problem because it is calculated by getting a harmonic mean of the precision and recall.

\begin{equation}
    F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = 2 \times \frac{precision \times recall}{precision + recall}
\end{equation}


Remaining part for the benchmarking involves choosing which algorithms to train on the dataset.
Given we don't know the distribution of the data, training fundamental machine learning algorithms together with the neural network algorithms is a cautious step to take. 
For that reason, I have chosen to train two fundamental machine learning algorithm, namely the Random Forest classifier and the Support vector classifier (SVC) for establishing a benchmark.

Previously I have mentioned in section \ref{sec:datalimitations} that the validation data for this dataset is very small.
Having only 16 records in validation dataset prevent reliable feedback from the validation metrics. 
For that reason, I had opted in for using the training dataset as a validation dataset and evaluated the model performance in both the test and the validation dataset at the end of the report.
Please note that the figures that will be reported in the next chapter are based on the validation scores of a single model.


\subsection{Random Forest Classifier}
For the past years, tree-based algorithms have been very popular in the academic community as well as in the industry with numerous papers demonstrated in ICML, NIPS and JMLR.
Random forest emerges in this category as a very strong algorithm by Breiman~\cite{randomforest} that achieves remarkable performance in small to a medium dataset.
Application of random forest involves choosing many hyper-parameters of this algorithm and 
the final baseline specification will be determined by finding an optimal number of estimators and maximum features using cross-validation.


\subsection{SVM Classifier}
In addition to Random Forest, the secondary mainstream machine learning algorithm is Support Victor Machine classifier~\cite{svm}.
The biggest factor for choosing this algorithm was its robustness in detecting non-linear features in the data using kernel trick.
Similarly to Random Forest hyper-parameters of this model will be determined with cross-validation.

\subsection{LeNet-5}
LeNet-5~\cite{Lenet5} is the first one of the well-known CNNs that I will apply to this problem to consider in baseline benchmark.
I kept the attributes of the network as close to the origin network as possible but some characteristic of the network had to be changed when it is trained on this classification problem.
The first difference in this project needed in input and output layers, MNIST dataset have ten classes therefore required ten dense neurons at the output layer in the original network but in this project, there are two classes to predict which can be achieved by having one dense output neuron instead.
Input layer also changed because $32 \times 32$ is not a suitable resolution for the pneumonia detection and changed to $224 \times 224$ for better representation.
Another change made to this model is using relu activation function rather than hyperbolic tangent function (tanh) because of the less than ideal gradient flow in saturated tanh function prevented model to converge to a good solution.
Details of the model summary given below for reference.

\begin{verbatim}
    Model: "LeNet-5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 220, 220, 6)       456       
_________________________________________________________________
average_pooling2d_4 (Average (None, 110, 110, 6)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 106, 106, 16)      2416      
_________________________________________________________________
average_pooling2d_5 (Average (None, 53, 53, 16)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 49, 49, 120)       48120     
_________________________________________________________________
flatten_2 (Flatten)          (None, 288120)            0         
_________________________________________________________________
dense_6 (Dense)              (None, 120)               34574520  
_________________________________________________________________
dense_7 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 85        
=================================================================
Total params: 34,635,761
Trainable params: 34,635,761
Non-trainable params: 0
_________________________________________________________________
\end{verbatim}

\subsection{AlexNet}
Similar to LeNet-5, AlexNet~\cite{Alexnet} also aimed to the kept original structure as much as possible.
Original AlexNet comprises of eight layers, five of those were convolution layers where some of them connected to max-pooling layer. 
Despite the fact, the architect is preserved almost same as the original implementation additional steps such as adding local response normalization or PCA augmentation is not applied because of the ad hoc nature of the process and limited effect of this processes in the final performance.
AlexNet is ultimately a very influential paper that steers the direction for how the CNNs are designed and fueled the adoption of the use of neural networks in many fields.
Fallowing quote from the paper also explains the reason why neural networks gain so much popularity and pushing the state of the art results year after year.
\begin{quote}
    \textit{"All of our experiments suggest that our results
can be improved simply by waiting for faster GPUs and bigger datasets to become available."}
\end{quote}

\begin{verbatim}
    Model: "AlexNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_10 (Conv2D)           (None, 54, 54, 96)        34944     
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 26, 26, 96)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 26, 26, 256)       614656    
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 12, 12, 256)       0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 12, 12, 384)       885120    
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 12, 12, 384)       1327488   
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 12, 12, 256)       884992    
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 5, 5, 256)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 6400)              0         
_________________________________________________________________
dense_6 (Dense)              (None, 4096)              26218496  
_________________________________________________________________
dropout_4 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dropout_5 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 4097      
=================================================================
Total params: 46,751,105
Trainable params: 46,751,105
Non-trainable params: 0
_________________________________________________________________
\end{verbatim}

\subsection{VGGNet}
VGGNet is one of the best performant in 2014 ILSVRC competition that gets the best performance in classification and localization task.
There are two different variations of this network is available namely the VGGNet 19 and VGGNet 16. 
For this project, 16 layered VGGNet 16 appears as a good choice because it has a sufficient capacity given that the dataset is relatively small and larger model likely to overfit to my project dataset.
The main contribution of the VGGNet is that it demonstrated the importance of the network depth to good performance.
The layer structure of the architecture is straightforward, only performs $3 \times 3$ convolution with $2 \times 2$ pooling.
Similarly, for the purpose of the benchmarking architecture kept as close to original as possible with only change is made to final layer replaced with one neuron dense layer to accommodate the binary classification task.
Advantage of using this network is that it is implemented in most of the modern software packages and is also available to initialize with the weights of imagenet. 
That property will be very useful for comparing the performance when transfer learning is explored in subsection \ref{subsec:transferlearning}.
Detail breakdown of the network listed below.

\begin{verbatim}
    Model: "VGGNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         [(None, 224, 224, 3)]     0         
_________________________________________________________________
vgg16 (Functional)           (None, 512)               14714688  
_________________________________________________________________
dense_3 (Dense)              (None, 4096)              2101248   
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dropout_3 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 4097      
=================================================================
Total params: 33,601,345
Trainable params: 33,601,345
Non-trainable params: 0

\end{verbatim}

% \subsection{ResNet}

\section{Improving Performance}
After establishing a benchmark for minimum acceptable performance, this step will focus on continuously improving the performance on the problem. 
Materialize that goal I have chosen two methods, applying transfer learning and building custom neural network.

\subsection{Transfer Learning} \label{subsec:transferlearning}
Transfer learning is fundamentally taking weights (or features) learned on one problem, and applying them to a similar or new problem that could benefit from it.
Transfer learning usually considered when the training data is too small to train a deep neural network from a scratch.
Process for transfer learning is simple, architecture is initialized with the weights learned from another problem without the top part of the model.
Here the top part is referred to the layer(s) at the end of the model which has got the high level features specific to the problem.
And given that the lower layers of the model contribute to simple features like vertical and horizontal lines or edges, weights in these layers held frozen to allow feature extraction from the images in new problem.
After that step, the top part of the model initialized with the random weights to be trained on the new dataset and start learning higher-level features from that data.
Transfer learning sometimes applied with the concept of \emph{fine-tuning}, which is a process of unfreezing the part of the convolution layers or the entire model and re-training it with the new data.

Because the dataset of this project is suffering from a small data problem, transfer learning is utilized to discover if generic datasets like imagenet~\cite{imagenet} can be helpful to improve the existing model.
Effectiveness of the transfer learning will be measured by training the VGG16 network with the imagenet weights and comparing the performance against the same network with a randomly initialized version from the benchmarking experiments.

\subsection{Custom Model Architecture}
Another method available to get higher performance in machine learning is to designing custom architecture that better represent the data.
% better reflect the data representation.
Despite the great potential of better performance, this method is usually more complex in nature because of the challenges in hyper-parameter search.
Modern CNNs have significantly more hyper-parameters compare to traditional machine learning algorithms. 
For instance, below items are a non-exhaustive list of hyper-parameters to consider when designing custom architecture.

\begin{itemize}
    \item Learning rate $\alpha$
    \item Momentum term $\beta$
    \item Number of layers
    \item Number of units in a layer
    \item Kernel size for convolution layers
    \item Type of the convolution layer
    \item Learning rate decay
    \item Type of optimization algorithm
    \item Regularization type and quantity
    \item Mini batch size
\end{itemize}

The complexity of the task gets significantly higher when the attributes such as convolution layers padding and strides included in the number of layer and kernel size.
When used with valid padding convolutional layer have addition constrain that the further layers have to be compatible with the output tensor shapes from previous layers.
Given these complexities and limited time frame of this project, some aspects of the hyper-parameter search will be constant to allow experimentation on other parameters.
Grid search and random search are two of the widely used method when determining the right hyper-parameters.
Studies suggest that random search is more efficient and computationally inexpensive than the grid search in the context of neural network~\cite{randomsearch}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/randomsearch.png}
    \caption{Random search versus grid search~\cite{randomsearch}}
    \label{fig:randomsearch}
\end{figure}

For the custom architecture, I will be running experiments with the random search over the following hyper-parameters.

\begin{itemize}
    \item Type of convolutional layer. Conv2D versus Depthwise convolution
    \item Number of layers
    \item Different convolution types (e.g. Depthwise convolution)
    \item Type of regularization (e.g. Batch normalization, Dropout)
\end{itemize}

\section{Model Interpretability}
Machine learning models and a more specifically neural network usually regarded as black-box machines.
Meaning that reason for their predictions are unknown.
However, there are a variety of techniques that design to elaborate the machine learning models emphasis points.
Before any model put into production it should also be examined for their predictive reasons and because of that, I added model interpretability rules before making a decision to promote any model to deployment stage.
One such technique call GradCAM~\cite{heatmap} is designed to highlight the areas in the image that contributes the most for the prediction machine learning outputs.

\subsection{GradCAM}
GradCAM~\cite{heatmap} is part of the general visualization technique called \emph{class activation map} (CAM).
It consists of creating a heatmap of activations over the input image space.
A class activation heatmap is a two dimensional surface of scores for individual class mapped into the corresponding location. For instance, if the input image is with pneumonia, heatmap indicates the areas that most represent pneumonia-like features found and vice versa for images that absent from pneumonia.
This technique originally designed for softmax output with different classes and to adapt it to binary classification with sigmoid output, but the integration to the sigmoid output is also possible because ultimately what algorithm needs is the probability of the certain class decision.
% additional logic based on assigning classification with probability grater that 0.5 to pneumonia presence and absence of it to those less than 0.5 probability.

\section{Deployments with CI/CD}
Upon discovery of the best performer model up to this point, if the model satisfies the interpretation stage, can be added to model folders as a candidate for production release.
Please note that due to the manual nature of the model interpretation stage this project utilizes continuous development rather than continuous deployment.
Because it is not possible to assess this step without having a human intuition in the loop.
% Because even though visualization for GradCAM can be generated in an automated fashion there is no measurable criteria available to asses that step.
After the insertion of the model to models folder, the test suite for the project will run and if the tests are successful, deployment script will automatically run to produce the model artificial needed for the deployment.
The final step in this process is to commit the most recent artifact to the central code repository for deployment.
This process repeats ever a time when there is a better performer model found.
Detailed information about the deployment choice and the assessment of the deployment is covered in chapter \ref{chap:deployment}.
